              precision    recall  f1-score   support

           A       0.92      1.00      0.96        11
           B       0.82      0.82      0.82        11
           C       0.90      0.82      0.86        11
           D       0.86      0.55      0.67        11
           E       0.89      0.73      0.80        11
           F       1.00      0.90      0.95        10
           G       1.00      0.80      0.89        10
           H       0.71      1.00      0.83        10
           I       0.91      1.00      0.95        10
           J       1.00      0.80      0.89        10
           K       1.00      0.80      0.89        10
           L       0.75      0.90      0.82        10
           M       1.00      1.00      1.00        10
           N       0.91      1.00      0.95        10
           O       0.83      0.50      0.62        10
           P       0.90      0.90      0.90        10
           Q       0.50      1.00      0.67        10
           R       1.00      0.80      0.89        10
           S       1.00      0.90      0.95        10
           T       0.80      0.80      0.80        10
           U       0.67      1.00      0.80        10
           V       1.00      1.00      1.00        10
           W       1.00      1.00      1.00        10
           X       1.00      0.90      0.95        10
           Y       1.00      0.80      0.89        10
           Z       1.00      1.00      1.00        10

    accuracy                           0.87       265
   macro avg       0.90      0.87      0.87       265
weighted avg       0.90      0.87      0.87       265


Precision: How many times the prediction was right (for each letter)
Recall: How many times the model was right when the real letter was this one (for each letter)
F1-Score: Harmonic mean between precision and recall
Support: Real samples (of each letter)
Macro Average: Metrics simple mean (all letters have the same weight)
Weighted Average: mean based on letter support
